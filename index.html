<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@400;500;600;700&display=swap" rel="stylesheet">
    <title>Block and Detail</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body {
            font-family: 'Noto Sans', sans-serif;
        }
    </style>
</head>

<body class="text-gray-800 grid place-items-center">
    <div class="pt-12 pb-16">

        <!-- Header -->
        <header>
            <div class="flex flex-col items-center justify-center space-y-4">
                <h1 class="text-4xl text-center font-semibold">Block and Detail: <br />Scaffolding Sketch-to-Image
                    Generation</h1>
                <div id="authors-info" class="flex flex-col items-center justify-center space-y-2">
                    <ul id=" authors-list" class="flex space-x-6 text-lg text-xl text-blue-400 ">
                        <li class="hover:underline hover:cursor-pointer"><a href="https://vsanimator.github.io/">Vishnu Sarukkai </a></li>
                        <li class="hover:underline hover:cursor-pointer"><a href="https://sylviayuan-sy.github.io/">Lu Yuan* </a></li>
                        <li class="hover:underline hover:cursor-pointer"><a href="https://www.mia-tang.com/">Mia Tang* </a> </li>
                        <li class="hover:underline hover:cursor-pointer"><a href="https://graphics.stanford.edu/~maneesh/"> Maneesh Agrawala </a> </li>
                        <li class="hover:underline hover:cursor-pointer"><a href="https://graphics.stanford.edu/~kayvonf/"> Kayvon Fatahalian </a> </li>
                    </ul>
                    <h2 class=" text-xl">Stanford University</h2>
                    <span class="text-sm text-slate-500"> *Denotes equal contribution </span>
                </div>

                <h2 class="text-3xl">UIST 2024</h2>
                <div id="link-btns-wrapper" class="grid place-items-center grid-cols-3">
                    <button class="bg-blue-50 border-2 border-blue-500 hover:bg-blue-500 hover:text-white text-blue-500 font-bold py-2 px-4 rounded">
                        <a href="" target="_blank">arXiv</a>
                    </button>
                    <button class="bg-blue-50 border-2 border-blue-500 hover:bg-blue-500 hover:text-white text-blue-500 font-bold py-2 px-4 rounded">
                        <a href="https://github.com/BlockDetail/Block-and-Detail" target="_blank">Code</a>
                    </button>
                    <button class="bg-blue-50 border-2 border-blue-500 hover:bg-blue-500 hover:text-white text-blue-500 font-bold py-2 px-4 rounded">
                        <a href="" target="_blank">Demo</a>
                    </button>
                </div>
            </div>
        </header>

        <!-- Main Content -->
        <main class="flex flex-col space-y-12 w-screen">
            <section class="basis-full flex flex-col items-center justify-center">
                <img src="assets/banner_robin.png" alt="Banner Image" class="max-w-screen-lg h-auto py-6" />
                <p class="max-w-screen-md text-slate-500">
                    Our sketch-to-image generation tool supports the iterative refinement process artists use to create images. Users can sketch <span class="text-green-600 font-bold">blocking strokes (green) </span>to specify coarse spatial composition and <span class="font-bold text-black">detail strokes (black)</span> to specify more precise silhouettes and shapes. At any point, users can generate high-fidelity image samples that loosely follow the spatial structure of <span class="text-green-600 font-bold">blocking strokes</span> portraying variations in the shape and proportions of the objects the user has
                    blocked out (e.g. the position and size of the robin after sketch 1, the placement of the branch after sketch 2.) </p>
            </section>
            <section class="flex flex-col items-center justify-center space-y-5 bg-slate-100 py-8">
                <h2 class="text-3xl font-semibold">Abstract</h2>
                <p class="text-slate-700 max-w-screen-md">
                    We introduce a novel sketch-to-image tool that aligns with the iterative refinement process of artists. Our tool lets users sketch <span class="text-green-600 font-bold"><b>blocking strokes</b></span> to coarsely represent the placement and form of objects and <span class="font-bold text-black">detail strokes</span> to refine their shape and silhouettes. We develop a two-pass algorithm for generating high-fidelity images from such sketches at any point in the iterative process.
                    <br /> <br />
                    In the first pass we use a ControlNet to generate an image that strictly follows all the strokes (blocking and detail) and in the second pass we add variation by renoising regions surrounding <span class="text-green-600 font-bold">blocking strokes</span>.
                    We also present a dataset generation scheme that, when used to train a ControlNet architecture, allows regions that do not contain strokes to be interpreted as not-yet-specified regions rather than empty space.
                    We show that this partial-sketch-aware ControlNet can generate coherent elements from partial sketches that only contain a small number of strokes.
                    <br /> <br />
                    The high-fidelity images produced by our approach serve as scaffolds that can help the user adjust the shape and proportions of objects or add additional elements to the composition.We demonstrate the effectiveness of our approach with a variety of examples and evaluative comparisons.
                </p>
            </section>

            <section class="flex flex-col items-center justify-center space-y-5">
                <h2 class="text-3xl font-semibold">Sequence Examples</h2>
                <h3 class="text-2xl font-semibold">House Example</h3>
                <img src="assets/results_house.png" alt="Results: House" class="max-w-screen-lg h-auto py-6" />
                <ul class="max-w-screen-md text-normal text-slate-500 space-y-5">
                    <li>
                        <b><i>Sketch 1: </i></b>Started by blocking out the general location and orientation of the house. Used the generated image samples 1 for further guidance on perspective and details.
                    </li>
                    <li>
                        <b><i>Sketch 2: </i></b> Chose a sample with good perspective, but decided to adjust the shape of the roof with detail strokes to make it taller and angled the base of the house with a detail stroke to match the perspective.
                    </li>
                    <li>
                        <b><i>Sketch 3: </i></b>Took inspiration from the generated samples 2 to add more windows using detail strokes and custom (not found in a sample) stair case railings with blocked in stairs.
                    </li>
                    <li>
                        <b><i>Sketch 4: </i></b>Saw the staircase in generated samples 3, but unsatisfied with the exact shape of it. Adjusted the shape using detail strokes to further refine railings and noticing a lack of entrance in the generated samples from sketch 3 added a doorway.
                    </li>
                    <li>
                        <b><i>Sketch 5: </i></b> Taking inspiration from one of the generated samples 4 to add more details including windows on the roof. Sketch 6: Observing that generated samples from sketch 5 offer scene completion options such as vegetation near the house, blocked out areas for the vegetation and foliage using blocking strokes. Satisfied with a generated result.
                </ul>
                <hr />
                <h3 class="text-2xl font-semibold">Scooter Example</h3>
                <img src="assets/results_scooter.png" alt="Results: Scooter" class="max-w-screen-lg h-auto py-6" />
                <p class="max-w-screen-md">
                    Sketch 1: Blocked out the rough shape of a scooter (in cartoon style) to get guidance from the system on size, form and proportions. Sketch 2: The generated samples from sketch 1 revealed better proportions for the wheels and better angles of the main curve marking out the body of the scooter. Traced one of the samples to lock down the scooter's general proportions and orientation. Left blank spaces for the tool to suggest more variations on the shape of the handles and the seat. Sketch 3: Looked through the generated samples from sketch 2, and traced the shape of a nice handle while moving it to a more desirable location with respect to the scooter body. Sketch 4: Used blocking strokes to roughly define the shape of the seat, taking inspiration from a generated sample from sketch 3. Sketch 5: Added custom rear-view mirrors that were not seen in any of the generated samples from sketch 4. Sketch 6: Added more detail strokes to the tires and to the seat and generated a desired result.
                </p>
                <hr />
                <h3 class="text-2xl font-semibold">Daisy Example</h3>
                <img src="assets/results_daisy.png" alt="Results: Daisy" class="max-w-screen-lg h-auto py-6" />
                <p class="max-w-screen-md">
                    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore
                    et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut
                    aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse
                    cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in
                    culpa qui officia deserunt mollit anim id est laborum.
                </p>

            </section>

            <section class="flex flex-col items-center justify-center space-y-5 bg-slate-100 py-8">
                <h2 class="text-3xl font-semibold">Methods</h2>
                <img src="assets/method_fig.png" alt="Banner Image" class="max-w-screen-lg h-auto py-6 drop-shadow" />
                <p class="max-w-screen-md">
                    Our algorithm takes as input a text prompt (‚Äúa baseball photorealistic‚Äù), and a sketch consisting of <span class="text-green-600 font-bold">blocking strokes (green) </span> and <span class="font-bold text-black">detail strokes (black)</span> . In a first pass it feeds all strokes to a partial-sketch-aware ControlNet to produce an image, denoted as ùêºùë°ùëê , that tightly adheres to all strokes. Here the contour of the baseball in ùêºùë°ùëê is misshapen because the input <span class="text-green-600 font-bold">blocking strokes</span> are not quite circular. To generate variation in areas surrounding the <span class="text-green-600 font-bold">blocking strokes</span>, our algorithm applies
                    a second diffusion pass we call Blended Renoising. Based on a renoising mask formed by dilating the input strokes, blended
                    renoising generates variation in the area surrounding <span class="text-green-600 font-bold">blocking strokes</span> while preserving areas near <span class="font-bold text-black">detail strokes</span> . The renoised
                    output image corrects the baseball's contour, while the location of the stitching closely follows the user's detailed strokes.
                </p>
            </section>

            <section class="flex flex-col items-center justify-center space-y-5">
                <h2 class="text-3xl font-semibold">BibTeX</h2>
                <p class="max-w-screen-md">
                    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore
                    et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut
                    aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse
                    cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in
                    culpa qui officia deserunt mollit anim id est laborum.
                </p>
            </section>

            <section class="flex flex-col items-center justify-center space-y-5">
                <h2 class="text-3xl font-semibold">Acknowledgements</h2>
                <p class="max-w-screen-md">
                    Support for this project was provided by Meta, Activision, Andreessen Horowitz and the Brown Institute for Media Innovation. Thank you to Joon Park, Purvi Goel, James Hong, Sarah Jobalia, Yingke Wang and Sofia Wyetzner for their feedback on our tool.
                </p>
            </section>
        </main>

    </div>
    <!-- Footer -->
    <footer class="bg-slate-100 w-screen flex items-center justify-center py-6">
        <p class="max-w-screen-md">
            Copyright information


            Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore
        </p>
    </footer>

</body>

</html>